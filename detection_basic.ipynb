{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORQQrPdzFItZZxilAOo6uR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuevu110405/AIO_Module_7/blob/feature%2Ftraining/detection_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c-u3gVw44yf"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the lastest dataset version\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(\"Path to dataset files:\" , data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights\n"
      ],
      "metadata": {
        "id": "svZTCvlW5emS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transforms = None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = self.filter_images_width_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) <= 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"Image {image_name} has multiple objects and will be excluded from the dataset\"\n",
        "                    )\n",
        "\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        #Annotation path\n",
        "        annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        #Parse annotation file\n",
        "        label = self.parse_annotation(annotation_path)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        label = None\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if(\n",
        "                label is None\n",
        "            ):\n",
        "                label = name\n",
        "\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else  -1\n",
        "\n",
        "        return label_num"
      ],
      "metadata": {
        "id": "vyXag2x28ftq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phan tich va chuan bi du lieu"
      ],
      "metadata": {
        "id": "-GwiN762Fzcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data directory\n",
        "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
        "image_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "#Get list of image files and create a dummy dataframe to split the data\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name' : image_files})\n",
        "\n",
        "#split data\n",
        "train_df, val_df = train_test_split(df, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "VxAopE4eF3Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform = transforms)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform = transforms)\n",
        "\n",
        "# Filter datasets based on train_df and val_df\n",
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
      ],
      "metadata": {
        "id": "_AO0ttz0HNl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "model = models.resnet18(weights = ResNet18_Weights.DEFAULT)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9_2AeGELm1r",
        "outputId": "b016fb5b-f532-439d-9ddd-c9f5a030e592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 167MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop\n",
        "num_epochs = 10\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data, targets in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            scores = model(data)\n",
        "            _, predictions = scores.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predictions == targets).sum()\n",
        "\n",
        "        print(f\"Epoch {epochs+1}/{num_epochs}, Accuracy: {100*correct/total:.2f}%\")"
      ],
      "metadata": {
        "id": "jq8-zlcNMeoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification + bounding box regression"
      ],
      "metadata": {
        "id": "W6FaaV5tVHEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(\"Path to dataset files:\" , data_dir)"
      ],
      "metadata": {
        "id": "ONN47wW_Ulg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights\n"
      ],
      "metadata": {
        "id": "KTOitFPgVFzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transforms = None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = self.filter_images_width_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) <= 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"Image {image_name} has multiple objects and will be excluded from the dataset\"\n",
        "                    )\n",
        "\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        #Annotation path\n",
        "        annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        #Parse annotation file\n",
        "        label, bbox = self.parse_annotation(annotation_path)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, label, bbox\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if(\n",
        "                label is None\n",
        "            ):\n",
        "                label = name\n",
        "                xmin = int(obj.find('bndbox/xmin').text)\n",
        "                ymin = int(obj.find('bndbox/ymin').text)\n",
        "                xmax = int(obj.find('bndbox/xmax').text)\n",
        "                ymax = int(obj.find('bndbox/ymax').text)\n",
        "\n",
        "                bbox = [\n",
        "                    xmin / image_width,\n",
        "                    ymin / image_height,\n",
        "                    xmax / image_width,\n",
        "                    ymax  / image_height\n",
        "\n",
        "                ]\n",
        "\n",
        "\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else  -1\n",
        "\n",
        "        return label_num, torch.tensor(bbox, dtype = torch.float32)\n"
      ],
      "metadata": {
        "id": "azDEQO1UWwZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
        "image_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name' : image_files})\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "iNNwmzlQZ7fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform = transforms)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform = transforms)\n",
        "\n",
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "#Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
      ],
      "metadata": {
        "id": "O07fYUsGeBVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model with two heads\n",
        "class TwoHeadedModel(nn.Module):\n",
        "    def __init__(self, num_classes = 2):\n",
        "        super(TwoHeadedModel, self).__init__()\n",
        "        self.base_model = models.resnet18(weights = ResNet18_Weights.DEFAULT)\n",
        "        self.num_ftrs = self.base_model.fc.in_features\n",
        "\n",
        "        self.base_model.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(self.num_ftrs, num_classes)\n",
        "        self.regressor = nn.Linear(self.num_ftrs, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        class_logits = self.classifier(x)\n",
        "        bbox_coords = torch.sigmoid(self.regressor(x))\n",
        "        return class_logits, bbox_coords"
      ],
      "metadata": {
        "id": "99togtxwdxjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "model = TwoHeadedModel()\n",
        "#device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "#Loss and optimizer\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_bbox = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n"
      ],
      "metadata": {
        "id": "k2kuljwxmD6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "num_epochs = 10\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets, bboxes) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        bboxes = bboxes.to(device)\n",
        "\n",
        "        scores, pred_bboxes = model(data)\n",
        "        loss_class = criterion_class(scores, targets)\n",
        "        loss_bbox = criterion_bbox(pred_bboxes, bboxes)\n",
        "        loss = loss_class + loss_bbox\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss_bbox = 0\n",
        "        total_samples = 0\n",
        "        for data, targets, bboxes in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "\n",
        "            scores, pred_bboxes = model(data)\n",
        "            _, predictions = scores.max(1)\n",
        "            correct += (predictions == targets).sum()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            total_loss_bbox += criterion_bbox(pred_bboxes, bboxes).item()*data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "        avg_loss_bbox = total_loss_bbox / total_samples\n",
        "\n",
        "        print(f\"Epoch {epochs+1}/{num_epochs}, Accuracy: {100*correct/total:.2f}%, Validation Loss: {total_loss_bbox/total_samples:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0KVrAXUNnZzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many objects classification and bounding box regression"
      ],
      "metadata": {
        "id": "uoiER9vGcFlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n"
      ],
      "metadata": {
        "id": "2Yg9MCJDcPVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transforms = None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = self.filter_images_width_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "\n",
        "\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "\n",
        "        for obj in root.findall(\"onject\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:\n",
        "                label = name\n",
        "\n",
        "                xmin = int(obj.find('bndbox/xmin').text)\n",
        "                ymin = int(obj.find('bndbox/ymin').text)\n",
        "                xmax = int(obj.find('bndbox/xmax').text)\n",
        "                ymax = int(obj.find('bndbox/ymax').text)\n",
        "\n",
        "                bbox = [\n",
        "                    xmin / image_width,\n",
        "                    ymin / image_height,\n",
        "                    xmax / image_width,\n",
        "                    ymax / image_height\n",
        "                ]\n",
        "\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "        return label_num, torch.tensor(bbox, dtype = torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_file = self.image_files[idx]\n",
        "        img1_path = os.path.join(self.image_dir, img1_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img1_file)[0] + \"xml\"\n",
        "        img1_annotations = self.parse_annotation(os.path.join(self.annotations_dir, annotation_name))\n",
        "\n",
        "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
        "        img2_file = self.image_files[idx2]\n",
        "        img2_path = os.path.join(self.image_dir, img2_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img2_file)[0] + \"xml\"\n",
        "        img2_annotations = self.parse_annotation(os.path.join(self.annotations_dir, annotation_name))\n",
        "\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        merged_image = Image.new(\"RGB\", (img1.width + img2.width, max(img1.height, img2.height)))\n",
        "\n",
        "        merged_image.paste(img1, (0, 0))\n",
        "        merged_image.paste(img2, (img1.width, 0))\n",
        "        merged_w = img1.width + img2.width\n",
        "        merged_h = max(img1.height, img2.height)\n",
        "\n",
        "        merged_annotations = []\n",
        "\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": img1_annotations[1].tolist(), \"label\" : img1_annotations[0]}\n",
        "        )\n",
        "\n",
        "        new_bbox = [\n",
        "            (img_annotations[1][0] * img2.width + img1.width) / merged_w,\n",
        "            (img_annotations[1][1] * img2.height) / merged_h,\n",
        "            (img_annotations[1][2] * img2.width + img1.width) / merged_w,\n",
        "            (img_annotations[1][3] * img2.height) / merged_h\n",
        "        ]\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": new_bbox, \"label\": img2_annotations[0]}\n",
        "        )\n",
        "\n",
        "        if self.transforms:\n",
        "            merged_image = self.transforms(merged_image)\n",
        "\n",
        "        return merged_image, merged_annotations\n",
        "\n"
      ],
      "metadata": {
        "id": "BMsxCqqKed9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data directory\n",
        "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
        "image_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name' : image_files})\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "1OgxECgGunsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform = transform)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform = transform)"
      ],
      "metadata": {
        "id": "vq8XhNeUvHfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)\n",
        "\n"
      ],
      "metadata": {
        "id": "MHsFo15CwOh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoHeadModel(nn.Module):\n",
        "    def __init__(self, num_classes = 2):\n",
        "        super(TwoHeadModel, self).__init__():\n",
        "        self.base_model = models.resnet18(weights = ResNet18_Weights.DEFAULT)\n",
        "        self.num_ftrs = self.base_model.fc.in_features\n",
        "\n",
        "        self.base_model.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(self.num_ftrs, num_classes)\n",
        "        self.regressor = nn.Linear(self.num_ftrs, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        class_logits = self.classifier(x)\n",
        "        bbox_coords = torch.sigmoid(self.regressor(x))\n",
        "        return class_logits, bbox_coords"
      ],
      "metadata": {
        "id": "A_6ecxNKyhAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoHeadModel()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_bbox = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n"
      ],
      "metadata": {
        "id": "_qYyzzgMz_K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets, bboxes) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        bboxes = bboxes.to(device)\n",
        "\n",
        "        scores, pred_bboxes = model(data)\n",
        "        loss_class = criterion_class(scores, targets)\n",
        "        loss_bbox = criterion_bbox(pred_bboxes, bboxes)\n",
        "        loss = loss_class + loss_bbox\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss_bbox = 0\n",
        "        total_sample = 0\n",
        "        for data, targets, bboxes in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "\n",
        "            scores, pred_bboxes = model(data)\n",
        "            _, predictions = score.max(1)\n",
        "            correct += (predictions == targets).sum()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            total_loss_bbox += criterion_bbox(pred_bboxes, bboxes).item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "\n",
        "        avg_loss_bbox = total_loss_bbox / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {100*correct/total:.2f}%, Validation Loss: {total_loss_bbox/total_samples:.4f}\")\n",
        "        print(f'AVG. bbox: {avg_loss_bbox:.4f}')"
      ],
      "metadata": {
        "id": "XuhAezkS00dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification(> 2 objects) + bounding box regression"
      ],
      "metadata": {
        "id": "CSlwgc3g5tC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n"
      ],
      "metadata": {
        "id": "XacIIRPi5sBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transforms = None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = self.filter_images_width_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "\n",
        "\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        image_width = int(root.find('size/width').text)\n",
        "        image_height = int(root.find('size/height').text)\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "\n",
        "        for obj in root.findall(\"onject\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:\n",
        "                label = name\n",
        "\n",
        "                xmin = int(obj.find('bndbox/xmin').text)\n",
        "                ymin = int(obj.find('bndbox/ymin').text)\n",
        "                xmax = int(obj.find('bndbox/xmax').text)\n",
        "                ymax = int(obj.find('bndbox/ymax').text)\n",
        "\n",
        "                bbox = [\n",
        "                    xmin / image_width,\n",
        "                    ymin / image_height,\n",
        "                    xmax / image_width,\n",
        "                    ymax / image_height\n",
        "                ]\n",
        "\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "        return label_num, torch.tensor(bbox, dtype = torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_file = self.image_files[idx]\n",
        "        img1_path = os.path.join(self.image_dir, img1_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img1_file)[0] + \"xml\"\n",
        "        img1_annotations = self.parse_annotation(os.path.join(self.annotations_dir, annotation_name))\n",
        "\n",
        "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
        "        img2_file = self.image_files[idx2]\n",
        "        img2_path = os.path.join(self.image_dir, img2_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img2_file)[0] + \"xml\"\n",
        "        img2_annotations = self.parse_annotation(os.path.join(self.annotations_dir, annotation_name))\n",
        "\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        merged_image = Image.new(\"RGB\", (img1.width + img2.width, max(img1.height, img2.height)))\n",
        "\n",
        "        merged_image.paste(img1, (0, 0))\n",
        "        merged_image.paste(img2, (img1.width, 0))\n",
        "        merged_w = img1.width + img2.width\n",
        "        merged_h = max(img1.height, img2.height)\n",
        "\n",
        "        merged_annotations = []\n",
        "\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": img1_annotations[1].tolist(), \"label\" : img1_annotations[0]}\n",
        "        )\n",
        "\n",
        "        new_bbox = [\n",
        "            (img_annotations[1][0] * img2.width + img1.width) / merged_w,\n",
        "            (img_annotations[1][1] * img2.height) / merged_h,\n",
        "            (img_annotations[1][2] * img2.width + img1.width) / merged_w,\n",
        "            (img_annotations[1][3] * img2.height) / merged_h\n",
        "        ]\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": new_bbox, \"label\": img2_annotations[0]}\n",
        "        )\n",
        "\n",
        "        if self.transforms:\n",
        "            merged_image = self.transforms(merged_image)\n",
        "        else:\n",
        "            merged_image = transforms.ToTensor()(merged_image)\n",
        "\n",
        "        annotations = torch.zeros((len(merged_annotations), 5))\n",
        "        for i, ann in enumerate(merged_annotations):\n",
        "            annotations[i] = torch.cat((torch.tensor([ann['bbox']]), torch.tensor(ann['label'])))\n",
        "\n",
        "        return merged_image, annotations\n"
      ],
      "metadata": {
        "id": "g_CnWOzmsvvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data directory\n",
        "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
        "image_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "#Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = MyDataset(annotations_dir, image_dir, transform = transform)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size = 0.2, random_state = 42)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle = False)"
      ],
      "metadata": {
        "id": "8AquX94vxwmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleYOLO, self).__init__():\n",
        "        self.backbone = models.resnet50(weights = ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "        self.fcs = nn.Linear(\n",
        "            2048, 2 * 2 * (4 + self.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
        "        features = features.view(features.size(0), -1)\n",
        "        output = self.fcs(features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "NolHXb66zFjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 2\n",
        "class_to_idx = {'dog' : 0, 'cat' : 1}\n",
        "\n",
        "model = SimpleYOLO(num_classes = num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "6KLDM_g42EME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(output, targets, device, num_classes):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_size = output.shape[0]\n",
        "    total_loss = 0\n",
        "\n",
        "    output = output.view(batch_size, 2, 2, 4 + num_classes)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(len(targets[i])):\n",
        "\n",
        "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
        "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
        "\n",
        "            grid_x = int(bbox_center_x * 2)\n",
        "            grid_y = int(bbox_center_y * 2)\n",
        "\n",
        "            label_one_hot = torch.zeros(num_classes, device = device)\n",
        "            label_one_hot[targets[i][j][4]] = 1\n",
        "\n",
        "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
        "\n",
        "            #2Regression loss for the responsible grid cell\n",
        "            bbox_target = targets[i][j][:4].to(device)\n",
        "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
        "\n",
        "            # 3 No object Loss(for other grid cells)\n",
        "            no_obj_loss = 0\n",
        "            for other_grid_y in range(2):\n",
        "                for other_grid_x in range(2):\n",
        "                    if other_grid_x != grid_x or other_grid != grid_y:\n",
        "                        no_obj_loss += mse_loss(output[i, other_grid_y, other_grid_x, :4], torch.zeros(4, device = device))\n",
        "\n",
        "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
        "\n",
        "    return total_loss / batch_size\n",
        "\n",
        "def evaluate_model(model, data_loader, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm.tqdm(data_loader, desc= 'Validation', leave = False):\n",
        "            images = images.to(device)\n",
        "            output = model(images)\n",
        "            loss = calculate_loss(output, targets, device, num_classes)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
        "\n",
        "            for batch_idx in range(images.shape[0]):\n",
        "                for target in targets[batch_idx]:\n",
        "                    bbox_center_x = (target[0] + target[2]) / 2\n",
        "                    bbox_center_y = (target[1] + target[3]) / 2\n",
        "\n",
        "                    grid_x = int(bbox_center_x * 2)\n",
        "                    grid_y = int(bbox_center_y * 2)\n",
        "\n",
        "                    prediction = output[batch_idx, grid_y, grid_x, :4].argmax().item()\n",
        "                    all_predictions.append(prediction)\n",
        "                    all_targets.append(target[4].item())\n",
        "    val_loss = running_loss / len(data_loader)\n",
        "\n",
        "    all_predictions = torch.tensor(all_predictions, device = device)\n",
        "    all_targets = torch.tensor(all_targets, device = device)\n",
        "\n",
        "    val_accuracy = (all_predictions == all_targets).float().mean().item()\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "axMkZi7C4rx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes):\n",
        "    best_val_accuracy = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(num_epochs), desc = \"Epochs\"):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "\n",
        "        for images, targets in tqdm.tqdm(train_loader, desc = 'Batches', leave = False):\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            train_losses.append(epoch_loss)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "            f\"Train Loss: {epoch_loss:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Accuracy: {val_accuracy:.4f}\"\n",
        "            )\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-MQEJW4TAFy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, image_path, transform, device, class_to_idx, threshold = 0.5):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_width, original_height = image.size\n",
        "\n",
        "    resized_image = image.resize((448, 448))\n",
        "    resized_width, resized_height = resized_image.size\n",
        "\n",
        "    transformed_image = transform(resized_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed_image)\n",
        "        output = output.view(1, 2, 2, 4 + len(class_to_idx))\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(resized_image)\n",
        "\n",
        "    for grid_y in range(2):\n",
        "        for grid_x in range(2):\n",
        "            class_pred = output[0, grid_y, grid_x, 4:].argmax().item()\n",
        "            bbox = output[0, grid_y, grid_x, :4].tolist()\n",
        "\n",
        "            confidence = torch.softmax(output[0, grid_y, grid_x, 4:], dim = 0)[class_pred].item()\n",
        "\n",
        "            x_min = bbox[0] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_min = bbox[1] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "            x_max = bbox[2] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_max = bbox[3] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "\n",
        "\n",
        "            if confidence > threshold:\n",
        "                rect = patches.Rectangle(\n",
        "                    (x_min, y_min),\n",
        "                    x_max - x_min,\n",
        "                    y_max - y_min,\n",
        "                    linewidth = 1,\n",
        "                    edgecolor = 'r',\n",
        "                    facecolor = 'none'\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "                plt.text(\n",
        "                    x_min,\n",
        "                    y_min,\n",
        "                    f\"{class_to_idx[class_pred]}: {confidence:.2f}\",\n",
        "                    color = 'white',\n",
        "                    fontsize = 12,\n",
        "                    bbox = dict(facecolor = 'red', alpha = 0.5)\n",
        "                )\n",
        "    plt.show()\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "image_path = \"mnt/c/Study/0D Project/good_1.jpg\"\n",
        "inference(model, image_path, transform, device, class_to_idx, threshold = 0.5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qs6FMAhcEazd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yolov1"
      ],
      "metadata": {
        "id": "0pHHeGL3MLUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
        "    def __init__(self, class_mapping, S = 7, B = 2, C = 20, custom_transforms = None):\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.class_mapping = class_mapping\n",
        "        self.custom_transforms = custom_transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
        "        img_width, img_height = image.size\n",
        "\n",
        "        bboxes = convert_to_yolo_format(target, img_width, img_height, self.class_mapping)\n",
        "        just_boxes = boxes[:, 1:]\n",
        "        labels = boxes[:, 0]\n",
        "\n",
        "        if self.custom_transforms:\n",
        "            sample = {\n",
        "                \"image\": image,\n",
        "                \"bboxes\": just_boxes,\n",
        "                \"labels\": labels\n",
        "            }\n",
        "            sample = self.custom_transforms(**sample)\n",
        "            image = sample[\"image\"]\n",
        "            bboxes = sample[\"bboxes\"]\n",
        "            labels = sample[\"labels\"]\n",
        "\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C+ 5 * self.B))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype = torch.float32)\n",
        "        labels = torch.tensor(labels, dtype = torch.float32)\n",
        "        image = torch.as_tensor(image, dtype = torch.float32)\n",
        "\n",
        "        for box, label in zip(boxes, labels):\n",
        "            x, y width, height = box.tolist()\n",
        "            class_label = label.item()\n",
        "\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "            width_cell, height_cell = (\n",
        "                width * self.S,\n",
        "                height * self.S\n",
        "            )\n",
        "            if label_matrix[i, j, 20] == 0:\n",
        "                label_matrix[i, j, 20] == 1\n",
        "                box_coordinates = torch.tensor(\n",
        "                    [x_cell, y_cell, width_cell, height_cell]\n",
        "                )\n",
        "                label_matrix[i, j, 21:25] = box_coordinates\n",
        "                label_matrix[i, j, class_label] = 1\n",
        "\n",
        "        return image, label_matrix"
      ],
      "metadata": {
        "id": "uZx7qdVpMK97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
        "    \"\"\"\n",
        "    Convert annotation data from VOC format to YOLO format.\n",
        "\n",
        "    Parameters:\n",
        "        target (dict): Annotation data from VOCDetection dataset.\n",
        "        img_width (int): Width of the original image.\n",
        "        img_height (int): Height of the original image.\n",
        "        class_mapping (dict): Mapping from class names to integer IDs.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of shape [N, 5] for N bounding boxes,\n",
        "            each with [x_center, y_center, width, height, class_id].\n",
        "    \"\"\"\n",
        "\n",
        "    annotations = target['annotation']['object']\n",
        "\n",
        "    # Ensure annotations is a list\n",
        "    if not isinstance(annotations, list):\n",
        "        annotations = [annotations]\n",
        "\n",
        "    boxes = []\n",
        "    for anno in annotations:\n",
        "        xmin = int(anno['bndbox']['xmin']) / real_width\n",
        "        xmax = int(anno['bndbox']['xmax']) / real_width\n",
        "        ymin = int(anno['bndbox']['ymin']) / real_height\n",
        "        ymax = int(anno['bndbox']['ymax']) / real_height\n",
        "\n",
        "        x_center = (xmin + xmax) / 2\n",
        "        y_center = (ymin + ymax) / 2\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        class_name = anno['name']\n",
        "        class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
        "\n",
        "        boxes.append([x_center, y_center, width, height, class_id])\n",
        "\n",
        "    return torch.tensor(boxes)"
      ],
      "metadata": {
        "id": "caJkIVFqLHbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) between bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        boxes_preds (torch.Tensor): Predicted bounding boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (torch.Tensor): Ground truth bounding boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): Box format, can be \"midpoint\" or \"corners\".\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Intersection over Union scores for each example.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to corners format if necessary\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "    elif box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid box_format. Choose 'midpoint' or 'corners'.\")\n",
        "\n",
        "    # Calculate coordinates of the intersection rectangle\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    # Compute the area of the intersection rectangle, clamp (0) to handle cases where they do not overlap\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    # Calculate the areas of the predicted and ground truth boxes\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    # Calculate the Intersection over Union, adding a small epsilon to avoid division by zero\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ],
      "metadata": {
        "id": "VYDBPIpKYBDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Performs Non-Maximum Suppression on a list of bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        bboxes (list): List of bounding boxes, each represented as\n",
        "                      [class_pred, prob_score, x1, y1, x2, y2].\n",
        "        iou_threshold (float): IoU threshold to determine correct predicted\n",
        "                              bounding boxes.\n",
        "        threshold (float): Threshold to discard predicted bounding boxes\n",
        "                           (independent of IoU).\n",
        "        box_format (str): \"midpoint\" or \"corners\" to specify the format of\n",
        "                          bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding boxes after performing NMS with a\n",
        "             specific IoU threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    # Filter bounding boxes based on probability threshold\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "\n",
        "    # Sort bounding boxes by probability in descending order\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        # Remove bounding boxes with IoU greater than the specified threshold\n",
        "        # with the chosen box\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0] or\n",
        "               intersection_over_union(\n",
        "                   torch.tensor(chosen_box[2:]),\n",
        "                   torch.tensor(box[2:]),\n",
        "                   box_format=box_format\n",
        "               ) < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms"
      ],
      "metadata": {
        "id": "2SV4-4rjYea6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5,\n",
        "                           box_format=\"midpoint\", num_classes=20):\n",
        "    \"\"\"\n",
        "    Calculate the mean average precision (mAP).\n",
        "\n",
        "    Args:\n",
        "        pred_boxes (list): A list containing predicted bounding boxes with each\n",
        "                          box defined as [train_idx, class_pred, prob_score,\n",
        "                          x1, y1, x2, y2].\n",
        "        true_boxes (list): Similar to pred_boxes but containing information\n",
        "                          about true boxes.\n",
        "        iou_threshold (float): IoU threshold, where predicted boxes are\n",
        "                              considered correct.\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify the format\n",
        "                          of the boxes.\n",
        "        num_classes (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        float: The mAP value across all classes with a specific IoU threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    average_precisions = []\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ],
      "metadata": {
        "id": "al_PR53gaEUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_config = {\n",
        "    (7, 64, 2, 3),\n",
        "    \"M\"\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\"\n",
        "    [(1, 512, 1, 0), (3, 512, 1, 1), 4],\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\"\n",
        "    [(1, 1024, 1, 0), (3, 1024, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "}\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias = False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "\n",
        "class Yolov1(nn.Module):\n",
        "    def __init__(self, in_channels = 3, **kwargs):\n",
        "        super(Yolov1, self).__init__()\n",
        "        self.architecture = architecture_config\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = self._create_conv_layers(self.architecture)\n",
        "        self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim = 1))\n",
        "\n",
        "    def _create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == tuple:\n",
        "                layers += [\n",
        "                    CNNBlock(\n",
        "                        in_channels, x[1], kernel_size = x[0], stride = x[2], padding = x[3]\n",
        "                    )\n",
        "                ]\n",
        "                in_channels = x[1]\n",
        "            elif type(x) == str:\n",
        "                layers += [nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))]\n",
        "\n",
        "            elif type(x) == list:\n",
        "                conv1 = x[0]\n",
        "                conv2 = x[1]\n",
        "                num_repeats = x[2]\n",
        "\n",
        "                for _ in range(num_repeats):\n",
        "                    layers += [\n",
        "                        CNNBlock(\n",
        "                            in_channels,\n",
        "                            conv1[1],\n",
        "                            kernel_size = conv1[0],\n",
        "                            stride = conv1[2],\n",
        "                            padding = conv1[3]\n",
        "                        )\n",
        "                    ]\n",
        "                    layers += [\n",
        "                        CNNBlock(\n",
        "                            conv1[1],\n",
        "                            conv2[1],\n",
        "                            kernel_size = conv2[0],\n",
        "                            stride = conv2[2],\n",
        "                            padding = conv2[3]\n",
        "                        )\n",
        "                    ]\n",
        "                    in_channels = conv2[1]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _create_fcs(self, split_size, num_boxes, num_classes ):\n",
        "        S, B, C = split_size, num_boxes, num_classes\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 496),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(496, S * S *(C + B * 5)),\n",
        "\n",
        "        )"
      ],
      "metadata": {
        "id": "RnrF99mE7lPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculate the loss for the YOLO (v1) model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, S=7, B=2, C=20):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        # Reshape predictions for easier indexing\n",
        "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
        "\n",
        "        # Calculate IoU for each bounding box prediction\n",
        "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
        "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "        # Get the box with the highest IoU\n",
        "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "        exists_box = target[..., 20].unsqueeze(3)\n",
        "\n",
        "        # ======================== #\n",
        "        # FOR BOX COORDINATES #\n",
        "        # ======================== #\n",
        "\n",
        "        # Choose the box with the highest IoU\n",
        "        box_predictions = exists_box * (\n",
        "            bestbox * predictions[..., 26:30]\n",
        "            + (1 - bestbox) * predictions[..., 21:25]\n",
        "        )\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "        # Square root of width and height\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "        )\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        box_loss = self.mse(\n",
        "            torch.flatten(box_predictions, end_dim=-2),\n",
        "            torch.flatten(box_targets, end_dim=-2),\n",
        "        )\n",
        "\n",
        "        # ==================== #\n",
        "        # FOR OBJECT LOSS #\n",
        "        # ==================== #\n",
        "\n",
        "        # Confidence score of the box with the highest IoU\n",
        "        pred_box = (\n",
        "            bestbox * predictions[..., 25:26]\n",
        "            + (1 - bestbox) * predictions[..., 20:21]\n",
        "        )\n",
        "\n",
        "        object_loss = self.mse(\n",
        "            torch.flatten(exists_box * pred_box),\n",
        "            torch.flatten(exists_box * target[..., 20:21]),\n",
        "        )\n",
        "\n",
        "        # ======================= #\n",
        "        # FOR NO OBJECT LOSS #\n",
        "        # ======================= #\n",
        "\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "\n",
        "        # ======================= #\n",
        "        # FOR CLASS LOSS #\n",
        "        # ======================= #\n",
        "\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n",
        "        )\n",
        "\n",
        "        # Calculate the final loss\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss\n",
        "        )\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "SPabwumVI95I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 300\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"yolov1.pth.tar\"\n",
        "\n"
      ],
      "metadata": {
        "id": "qf10I4hzL4ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH = 448\n",
        "HEIGHT = 448\n",
        "\n",
        "def get_train_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.HueSaturationValue(\n",
        "                        hue_shift_limit=0.2,\n",
        "                        sat_shift_limit=0.2,\n",
        "                        val_shift_limit=0.2,\n",
        "                        p=0.9\n",
        "                    ),\n",
        "                    A.RandomBrightnessContrast(\n",
        "                        brightness_limit=0.2,\n",
        "                        contrast_limit=0.2,\n",
        "                        p=0.9\n",
        "                    ),\n",
        "                ],\n",
        "                p=0.9\n",
        "            ),\n",
        "            A.ToGray(p=0.01),\n",
        "            A.HorizontalFlip(p=0.2),\n",
        "            A.VerticalFlip(p=0.2),\n",
        "            A.Resize(height=WIDTH, width=WIDTH, p=1),\n",
        "            # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        p=1.0,\n",
        "        bbox_params=A.BboxParams(\n",
        "            format='yolo',\n",
        "            min_area=0,\n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=WIDTH, width=WIDTH, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        p=1.0,\n",
        "        bbox_params=A.BboxParams(\n",
        "            format='yolo',\n",
        "            min_area=0,\n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "feeTC2b_Mx4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {\n",
        "    'aeroplane': 0,\n",
        "    'bicycle': 1,\n",
        "    'bird': 2,\n",
        "    'boat': 3,\n",
        "    'bottle': 4,\n",
        "    'bus': 5,\n",
        "    'car': 6,\n",
        "    'cat': 7,\n",
        "    'chair': 8,\n",
        "    'cow': 9,\n",
        "    'diningtable': 10,\n",
        "    'dog': 11,\n",
        "    'horse': 12,\n",
        "    'motorbike': 13,\n",
        "    'person': 14,\n",
        "    'pottedplant': 15,\n",
        "    'sheep': 16,\n",
        "    'sofa': 17,\n",
        "    'train': 18,\n",
        "    'tvmonitor': 19\n",
        "}"
      ],
      "metadata": {
        "id": "v_8h9Pi5SPsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        model (nn.Module): The model to train.\n",
        "        optimizer (Optimizer): The optimizer used for training.\n",
        "        loss_fn (callable): The loss function.\n",
        "        epoch (int): The current epoch number.\n",
        "\n",
        "    Returns:\n",
        "        float: Average mAP for the epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_maps = []\n",
        "    total_batches = len(train_loader)\n",
        "    display_interval = total_batches // 5\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_boxes, true_boxes = get_bboxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "        mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_maps.append(mAP.item())\n",
        "\n",
        "        if batch_idx % display_interval == 0 or batch_idx == total_batches - 1:\n",
        "            print(f\"Epoch: {epoch:3d} \\t Iter: {batch_idx:3d}/{total_batches:3d} \\t Loss: {loss.item():.10f} \\t mAP: {mAP.item():.10f}\")\n",
        "\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    avg_mAP = sum(epoch_maps) / len(epoch_maps)\n",
        "    print(colored(f\"Train \\t loss: {avg_loss:.10f} \\t mAP: {avg_mAP:.10f}\", 'green'))\n",
        "\n",
        "    return avg_mAP\n",
        "\n",
        "def test_fn(test_loader, model, loss_fn, epoch):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test set.\n",
        "\n",
        "    Args:\n",
        "        test_loader (DataLoader): DataLoader for test data.\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        loss_fn (callable): The loss function.\n",
        "        epoch (int): The current epoch number.\n",
        "\n",
        "    Returns:\n",
        "        float: Average mAP for the epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_maps = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, y)\n",
        "\n",
        "            pred_boxes, true_boxes = get_bboxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "            mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_maps.append(mAP.item())\n",
        "\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    avg_mAP = sum(epoch_maps) / len(epoch_maps)\n",
        "    print(colored(f\"Test \\t loss: {avg_loss:.10f} \\t mAP: {avg_mAP:.10f}\", 'yellow'))\n",
        "\n",
        "    model.train()\n",
        "    return avg_mAP"
      ],
      "metadata": {
        "id": "TcvH41npTwHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    Trains the YOLOv1 model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize model, optimizer, loss\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = YoloLoss()\n",
        "\n",
        "    # Load checkpoint if necessary\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='train',\n",
        "        download=True,\n",
        "    )\n",
        "    train_dataset.init_config_yolo(class_mapping=class_mapping,\n",
        "                                   custom_transforms=get_train_transforms())\n",
        "\n",
        "    testval_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='val',\n",
        "        download=True,\n",
        "    )\n",
        "    testval_dataset.init_config_yolo(class_mapping=class_mapping,\n",
        "                                    custom_transforms=get_val_transforms())\n",
        "\n",
        "    # Split testval dataset into validation and test sets\n",
        "    dataset_size = len(testval_dataset)\n",
        "    val_size = int(0.15 * dataset_size)\n",
        "    test_size = dataset_size - val_size\n",
        "    val_indices = list(range(val_size))\n",
        "    test_indices = list(range(val_size, val_size + test_size))\n",
        "\n",
        "    # Create samplers\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=val_sampler,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=test_sampler,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    best_mAP_train = 0\n",
        "    best_mAP_val = 0\n",
        "    best_mAP_test = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n",
        "        val_mAP = test_fn(val_loader, model, loss_fn, epoch)\n",
        "        test_mAP = test_fn(test_loader, model, loss_fn, epoch, is_test=True)\n",
        "\n",
        "        # Update best mAP values\n",
        "        best_mAP_train = max(best_mAP_train, train_mAP)\n",
        "        best_mAP_val = max(best_mAP_val, val_mAP)\n",
        "        best_mAP_test = max(best_mAP_test, test_mAP)\n",
        "\n",
        "        # Save checkpoint when validation mAP improves\n",
        "        if val_mAP > best_mAP_val:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
        "\n",
        "    print(colored(f\" Best Train mAP: {best_mAP_train:.10f}\", 'green'))\n",
        "    print(colored(f\" Best Val mAP: {best_mAP_val:.10f}\", 'blue'))\n",
        "    print(colored(f\" Best Test mAP: {best_mAP_test:.10f}\", 'yellow'))"
      ],
      "metadata": {
        "id": "V92rkxLaVYyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}